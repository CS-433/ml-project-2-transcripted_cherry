{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01148799-b3bb-4ded-856e-5dedb7af0eba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1172,
     "status": "ok",
     "timestamp": 1670264490534,
     "user": {
      "displayName": "Alexis Janin",
      "userId": "06183722824926644012"
     },
     "user_tz": -60
    },
    "id": "fIH5zV82ikCD",
    "outputId": "17045f43-dc86-4a45-fc64-9b6c09593125"
   },
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b092c-d3e8-407b-8f69-72bdf0ab24d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1670264494610,
     "user": {
      "displayName": "Alexis Janin",
      "userId": "06183722824926644012"
     },
     "user_tz": -60
    },
    "id": "PMX6b1fuik0W",
    "outputId": "53efa645-7fcc-4217-9f85-0d84e08cd5c4"
   },
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad331a-ceec-4bb8-9432-129b0f7d3ae3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5079,
     "status": "ok",
     "timestamp": 1670265864034,
     "user": {
      "displayName": "Alexis Janin",
      "userId": "06183722824926644012"
     },
     "user_tz": -60
    },
    "id": "vTvz9Ho5SDFV",
    "outputId": "38a6f9e2-8c74-4518-beff-c509c89c4041"
   },
   "source": [
    "!pip install cellpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a6448-9985-4690-8370-a9f26b245f9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10142,
     "status": "ok",
     "timestamp": 1670266045675,
     "user": {
      "displayName": "Alexis Janin",
      "userId": "06183722824926644012"
     },
     "user_tz": -60
    },
    "id": "kJPw7TghSUVR",
    "outputId": "9dcaab09-36a2-45f0-a3a4-609ec55af920"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7880d81b-28e4-4e28-9d81-bb5833bd69c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.20 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11236/4175053443.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcellpose\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcellpose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcellpose\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cellpose\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodels_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdynamics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnetModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massign_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_mkl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_model_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cellpose\\transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtransforms_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdynamics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_taper_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cellpose\\dynamics.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtifffile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnjit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfastremap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numba\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[0m_ensure_llvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m \u001b[0m_ensure_critical_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;31m# we know llvmlite is working as the above tests passed, import it now as SVML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numba\\__init__.py\u001b[0m in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Numba needs NumPy 1.17 or greater\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnumpy_version\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Numba needs NumPy 1.20 or less\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 1.20 or less"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import time, os, sys, random\n",
    "from urllib.parse import urlparse\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from cellpose import models\n",
    "from cellpose.io import imread\n",
    "from cellpose import plot\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd976f8-c328-411d-860b-995d6c65feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import ndimage as ndi\n",
    "import cv2\n",
    "from skimage.morphology import square, dilation\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "# https://stackoverflow.com/questions/19970764/making-feature-vector-from-gabor-filters-for-classification\n",
    "# https://stackoverflow.com/questions/20608458/gabor-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db57d46-5502-4c69-b3e5-77ed104caf32",
   "metadata": {},
   "source": [
    "# Image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75770f1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27389,
     "status": "ok",
     "timestamp": 1670266220515,
     "user": {
      "displayName": "Alexis Janin",
      "userId": "06183722824926644012"
     },
     "user_tz": -60
    },
    "id": "75770f1e",
    "outputId": "87f32cc7-ce8d-462e-ec04-f0e03f941c3d"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "# Number of images we want to read. Only in test phase. If we want to read all images we will use len(Cherry_file) in for loop below\n",
    "nb_images = 2\n",
    "\n",
    "# Get path to folders containing images\n",
    "Image_Directory = os.getcwd() + \"\\\\20151218_E14_BM_mTF8additionals_4\"\n",
    "#Image_Directory = rC:\\Users\\Claudia\\Desktop\\Master EPFL\\machine learning\\project_2  #Easier for me (Alexis) than finding of to get my path file nicely\n",
    "#Image_Directory = \"/content/gdrive/MyDrive/ML_2_transcripted_cherry/TF1\"\n",
    "# other folders: \n",
    "# \\\\20151218_E14_BM_mTF8additionals_4\n",
    "\n",
    "print(Image_Directory)\n",
    "\n",
    "# Calculate number of document-> array containing \n",
    "\n",
    "# Create Lists containing datapath to all images of Cherry/ Ypet files\n",
    "image_format = \"tif\"\n",
    "cherry_condition = \"TexasRed\"\n",
    "ypet_condition = \"YFP\"\n",
    "Cherry_file = []\n",
    "Ypet_file = []\n",
    "test = []\n",
    "TF_name_tot = []\n",
    "\n",
    "# Walk through directory (call all files) and assign them to proper list\n",
    "for root, dirs, files in os.walk(Image_Directory, topdown=False): # root = given in Input, dirs = folders, files= files\n",
    "    for f in files:\n",
    "        if f.endswith(image_format) and cherry_condition in f: \n",
    "            test.append(f)\n",
    "            TF_name_tot.append(f.partition(\"(\")[0])\n",
    "            Cherry_file.append(os.path.join(root, f))\n",
    "        elif f.endswith(image_format) and ypet_condition in f: \n",
    "            Ypet_file.append(os.path.join(root, f))\n",
    "\n",
    "#assert len(Cherry_file) != len(Ypet_file), f\"There are {len(Cherry_file)} Cherry files and {len(Ypet_file)} Ypet files. The number has to be equal, check manually what's missing\"\n",
    "\n",
    "\n",
    "# Initialize lists containing cherry, ypet images. Length = nb_images or len(Cherry_file) if we want to read all images\n",
    "cherry_imgs = np.ndarray(nb_images,dtype = np.ndarray)\n",
    "ypet_imgs = np.ndarray(nb_images,dtype = np.ndarray)\n",
    "\n",
    "\n",
    "# Read image (We donnot add it in for loop above to enable reading subset of image)\n",
    "for i in range(nb_images): #len(Cherry_file):\n",
    "    cherry_imgs[i] = imread(Cherry_file[i])\n",
    "    ypet_imgs[i] = imread(Ypet_file[i])\n",
    "    \n",
    "cherry_imgs = cherry_imgs.tolist()\n",
    "ypet_imgs = ypet_imgs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d67a3-1eb4-47a0-9d01-7c4f3d680639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imread(Ypet_file[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71c78e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2080427,
     "status": "ok",
     "timestamp": 1670268481424,
     "user": {
      "displayName": "Alexis Janin",
      "userId": "06183722824926644012"
     },
     "user_tz": -60
    },
    "id": "1a71c78e",
    "outputId": "2712c8bf-2566-4b9f-a390-3843e97a681d"
   },
   "outputs": [],
   "source": [
    "# Definine parameters for cellpose module using nuclei modus\n",
    "# https://cellpose.readthedocs.io/en/latest/settings.html\n",
    "channels = [0,0] # Corresponds to greyscale image\n",
    "diameter = None # Model was trained on diameters of 17 pixels. None-> automated estimation of the diameter. Can be changed by hand later\n",
    "flow_threshold = 0.4 # maximum allowed error of the flows for each mask, default = 0.4\n",
    "gpu = False\n",
    "model_type='nuclei'\n",
    "\n",
    "# Set model\n",
    "model = models.Cellpose(gpu, model_type)\n",
    "\n",
    "# Nuclei segmentation of all images within cherry_imgs list\n",
    "masks, flows, styles, diams = model.eval(cherry_imgs, diameter=diameter, channels=channels, flow_threshold=flow_threshold, do_3D=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K5e-n0lEosJH",
   "metadata": {
    "executionInfo": {
     "elapsed": 15884,
     "status": "ok",
     "timestamp": 1670269123554,
     "user": {
      "displayName": "Alexis Janin",
      "userId": "06183722824926644012"
     },
     "user_tz": -60
    },
    "id": "K5e-n0lEosJH"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from cellpose import io\n",
    "import os\n",
    "#os.mkdir(\"/content/gdrive/MyDrive/ML_2_transcripted_cherry/masks_TF1/A_1\")\n",
    "files_names = [\"/content/gdrive/MyDrive/ML_2_transcripted_cherry/masks_TF1/A_1/img_\"+str(i+1) for i in range(nb_images)]\n",
    "io.masks_flows_to_seg(cherry_imgs, masks, flows, diams, files_names, channels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39267371",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 15912,
     "status": "ok",
     "timestamp": 1670263505470,
     "user": {
      "displayName": "Alexis Janin",
      "userId": "06183722824926644012"
     },
     "user_tz": -60
    },
    "id": "39267371",
    "outputId": "d8b75fe6-2db0-4264-ddbd-ca12fa06d001"
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "image_i = 0 # Number of image we want to display. Max value= nb_images or len(Cherry_file)\n",
    "fig = plt.figure(figsize=(24,8))\n",
    "plot.show_segmentation(fig, cherry_imgs[image_i], masks[image_i], flows[image_i][0], channels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdd4de-9da7-4418-a714-258c71e9e1cd",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465a7ed-0db3-4384-9abc-dfa8208579d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_feature_extraction(kernels,patch_data):\n",
    "    # Apply Gabor filter: We receive per kernel a response. We stack all the response to one matrix---------------------------------------------------------\n",
    "    image = patch_data\n",
    "    g_response = []\n",
    "\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "        g_response.append(filtered)\n",
    "    n_clusters = len(g_response)\n",
    "    g_response = np.vstack(g_response)\n",
    "\n",
    "    # Cluster Gabor response with k-means. Find which part of the pixel belongs to which Gabor response ---------------------------------------------------------\n",
    "    random_state = 1\n",
    "    random.seed(random_state)\n",
    "    kmean = KMeans(n_clusters, random_state=random_state).fit(g_response) #\n",
    "\n",
    "    # Texton map: Assign each pixel to the identity of the closest cluster center---------------------------------------------------------\n",
    "    histogram = np.histogram(kmean.labels_, bins = len(np.unique(kmean.labels_)))\n",
    "    # Normalize\n",
    "    gabor_feature = histogram[0] / histogram[0].sum()\n",
    "    return gabor_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360298d-69df-4362-b7f3-dc2c2fe04476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_binary_pattern(patch_data):\n",
    "    # Apply lbp\n",
    "    METHOD = 'uniform' #  'default'\n",
    "    radius = 1\n",
    "    n_points = 8 * radius\n",
    "    lbp = local_binary_pattern(patch_data, n_points, radius, METHOD)\n",
    "    \n",
    "    # Create histogram\n",
    "    count = len(np.unique(lbp))\n",
    "    H, bins = np.histogram(lbp, count)\n",
    "\n",
    "    # Drop background values\n",
    "    w = width = radius - 1\n",
    "    index_background = n_points - w\n",
    "    H = H[0: index_background]\n",
    "    bins =  bins[0: index_background]\n",
    "\n",
    "    # plt.bar(bins,H,width=1)\n",
    "    \n",
    "    # Normalize\n",
    "    lbp_feature = H / H.sum()\n",
    "    return lbp_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d82b60-812a-40a1-94c0-d2211ee97863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.color import label2rgb\n",
    "from skimage.transform import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0dc673-770f-4a72-bcb5-a12ff16f3383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list of features \n",
    "columnname = [\"\" for i in range(1)]\n",
    "patch_size = [0 for i in range(1)]\n",
    "mean_intensity = [0 for i in range(1)]\n",
    "sum_intensity = [0 for i in range(1)]\n",
    "median_intensity = [0 for i in range(1)]\n",
    "variance_intensity = [0 for i in range(1)]\n",
    "imgs = [-1 for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8dcc4c-814f-40a5-94d2-cb78fa415d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Gabor filter bank kernels---------------------------------------------------------\n",
    "kernels = []\n",
    "sigmas = [1,3]\n",
    "tilts = [0,0.25,0.5,0.75] # perpendicular, diagonal, horizontal, other diagonal \n",
    "Wavelengths = [0.25,0.75] \n",
    "gamma =  0.5 # Spatial aspect ratio\n",
    "psi = 0 # Phase offset.\n",
    "\n",
    "for tilt in tilts:\n",
    "    for sigma in sigmas:\n",
    "        for lambd in Wavelengths:\n",
    "            kernel = cv2.getGaborKernel((10,10), sigma, tilt, lambd, gamma, psi, ktype = cv2.CV_32F)\n",
    "            kernels.append(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4b125-5c04-4fe7-9bf7-383e7672b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23495d64-fd3d-4f6d-9ab9-ff72862f41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature of each nucleus within images nb_images\n",
    "list_of_all_images = []\n",
    "\n",
    "for j in range(nb_images):\n",
    "    \n",
    "    # Initialize features: Faster then appending values to list with append\n",
    "    mask =  masks[j]  \n",
    "    TF_name = TF_name_tot[j]\n",
    "    start_nucleus = 10 #1 # we have to at least start from 1, otherwise we take whole mask\n",
    "    end_nucleus =  20 #len(np.unique(mask)) # len(np.unique(mask))  #IMAGE SPECIFIC: cases where cell are big enough (sell last cell of the notebook)\n",
    "    range_s = range(start_nucleus, end_nucleus)\n",
    "    imgs = [j for i in range_s]\n",
    "    ypet = ypet_imgs[j]\n",
    "    columnname = [\"\" for i in range_s]\n",
    "    blur = [False for i in range_s]\n",
    "    patch_size = [0 for i in range_s]\n",
    "    mean_intensity = [0 for i in range_s]\n",
    "    sum_intensity = [0 for i in range_s]\n",
    "    median_intensity = [0 for i in range_s]\n",
    "    std_intensity = [0 for i in range_s]\n",
    "    var_intensity = [0 for i in range_s]\n",
    "    skew_intesity = [0 for i in range_s]\n",
    "    kurt_intesity = [0 for i in range_s]\n",
    "    iqr_intesity = [0 for i in range_s]\n",
    "    entropy_intesity = [0 for i in range_s]\n",
    "    gabor_features = [0 for i in range_s]\n",
    "    lbp_features = [0 for i in range_s]\n",
    "    blur_lapl = [False for i in range_s]\n",
    "    blur_ski = [False for i in range_s]\n",
    "                  \n",
    "    #calculate different properties of the mask\n",
    "    props = regionprops(mask)\n",
    "    \n",
    "    #initialize arrays to store information from the mask\n",
    "    perimeter = [0 for i in range_s]\n",
    "    feret_diameter_max = [0 for i in range_s]\n",
    "    area_test = [0 for i in range_s]\n",
    "    axis_major = [0 for i in range_s]\n",
    "    axis_minor = [0 for i in range_s]\n",
    "    solidity = [0 for i in range_s]\n",
    "\n",
    "    '''\n",
    "    # Iterate to explore and find good threshold in exploration phase\n",
    "    patch_data_list = []\n",
    "    variance_data_list = []'''\n",
    "    # Threshold for blur detection\n",
    "    var_threshold = 10000000\n",
    "    blur_threshold = 0.7\n",
    "    \n",
    "    # Iterate through all nucleus: Take submatrix, Calculate patch size and corresponding ypet signal (mean) and store it into ypet_intensity\n",
    "    iter_i = 0 # Ensures that no 0 are saved where patch is too small/big\n",
    "    \n",
    "    for i in range_s:# # Iterate through all nucleus: in range(start_nucleus, end_nucleus)\n",
    "        cur = ypet[mask==i]\n",
    "        patch_size_ = np.count_nonzero(~np.isnan(cur))\n",
    "        \n",
    "        # Extract subimage ----------------------------------------------------------------\n",
    "        # Extract subimage of nucleus (Not just its values, which is done above)\n",
    "        boolean = np.where(mask==i, 1,False) # Boolean matrix where patch = 1, else 0\n",
    "        patch_values = ypet*boolean\n",
    "        # Extract submatrix with a buffer of 3 pixels to calculate the blurrness\n",
    "        variance_boolean = dilation(boolean, square(7))\n",
    "        variance_values = ypet * variance_boolean\n",
    "        \n",
    "        # Drop pixels without nucleus information ----------------------------------------------------------------\n",
    "        # Initialize conditions on when a column/ row should be dropped\n",
    "        drop_nucleus = False\n",
    "        drop_column = []\n",
    "        drop_column_variance = []\n",
    "        drop_row = []\n",
    "        drop_row_variance = []\n",
    "        \n",
    "        \n",
    "        # Iterate through all columns to find nucleus that is on the border and the columns that only contain mask\n",
    "        for column in range(boolean.shape[1]):\n",
    "            \n",
    "            # drop nucleus if it's on the border \n",
    "            if (((column == 0) | (column == boolean.shape[1]-1)) & (np.any(boolean[:,column])== True)):\n",
    "                drop_nucleus = True\n",
    "                break \n",
    "                \n",
    "            # drop if column has no information about nucleus: first check for broader variance_submatrix (Matrix with a buffer around submatrix to calculate variance). Then smaller submatrix \n",
    "            if np.any(variance_boolean[:,column])== False:\n",
    "                drop_column_variance.append(column) \n",
    "                drop_column.append(column) \n",
    "            elif np.any(boolean[:,column])== False: \n",
    "                drop_column.append(column) \n",
    "        \n",
    "        # Iterate through all columns to find nucleus that is on the border and the columns that only contain mask\n",
    "        for row in range(boolean.shape[0]):\n",
    "            \n",
    "            # drop nucleus if it's on the border \n",
    "            if (((row == 0) | (row == boolean.shape[0]-1)) & (np.any(boolean[row,:])== True)):\n",
    "                drop_nucleus = True\n",
    "                break\n",
    "                \n",
    "            # drop if row has no information about nucleus:  first check for broader variance_submatrix  \n",
    "            if np.any(variance_boolean[row,:])== False:\n",
    "                drop_row.append(row)\n",
    "                drop_row_variance.append(row)\n",
    "            elif np.any(boolean[row,:])== False: \n",
    "                drop_row.append(row)\n",
    "        \n",
    "        # Drop pixels without nucleus information\n",
    "        patch_data = np.delete(patch_values,drop_column,1)  \n",
    "        patch_data = np.delete(patch_data,drop_row,0) \n",
    "        variance_data = np.delete(ypet,drop_column_variance,1)  \n",
    "        variance_data = np.delete(variance_data,drop_row_variance,0) \n",
    "        \n",
    "        '''\n",
    "        # Exploration phase\n",
    "        patch_data_list.append(patch_data)\n",
    "        variance_data_list.append(variance_data)'''\n",
    "        \n",
    "        # Calculate blurness based on laplacian----------------------------------------------------------------\n",
    "        var_lapl = cv2.Laplacian(variance_data, cv2.CV_64F).var()\n",
    "        if var_lapl >= var_threshold: # nucleus is blurry\n",
    "            blured_nucleus = True\n",
    "        else: \n",
    "            blured_nucleus = False\n",
    "        \n",
    "        # Calculate blurness based on skimage----------------------------------------------------------------\n",
    "        blur_ski_nucleus = measure.blur_effect(variance_data)\n",
    "        if blur_ski_nucleus >= blur_threshold:\n",
    "            blur_ski_temp = True\n",
    "        else:\n",
    "            blur_ski_temp = False\n",
    "        \n",
    "        # Feature calculation ----------------------------------------------------------------\n",
    "        \n",
    "        if ((patch_size_ > 10) & (patch_size_ < 50000) & (drop_nucleus== False)): \n",
    "            # extract intensity feature\n",
    "            columnname[iter_i] = \"patch_\" + str(iter_i)    #We should not forgot that there is a -1 here. (when it is here)               \n",
    "            patch_size[iter_i] = patch_size_\n",
    "            blur_lapl[iter_i] = blured_nucleus\n",
    "            blur_ski[iter_i] = blur_ski_temp\n",
    "            sum_intensity[iter_i] = np.sum(np.sum(cur))\n",
    "            mean_intensity[iter_i] = np.nanmean(cur)\n",
    "            median_intensity[iter_i] = np.nanmedian(cur)\n",
    "            \n",
    "            std_intensity[iter_i] = np.nanstd(cur)\n",
    "            var_intensity[iter_i] = np.nanvar(cur)\n",
    "            skew_intesity[iter_i] = scipy.stats.skew(cur)\n",
    "            kurt_intesity[iter_i] = scipy.stats.kurtosis(cur)\n",
    "            iqr_intesity[iter_i] = scipy.stats.iqr(cur)\n",
    "            entropy_intesity[iter_i] = scipy.stats.entropy(cur)\n",
    "            \n",
    "            gabor_features[iter_i] = gabor_feature_extraction(kernels,patch_data) # List of two arrays, first nb of counts in histogram, second: bin number\n",
    "            temp_lbp = feature_binary_pattern(patch_data)\n",
    "            lbp_features[iter_i]  = temp_lbp\n",
    "            lbp_count = len(temp_lbp)\n",
    "            \n",
    "            perimeter[iter_i] = props[i-1].perimeter\n",
    "            feret_diameter_max[iter_i] = props[i-1].feret_diameter_max\n",
    "            area_test[iter_i] = props[i-1].area\n",
    "            axis_major[iter_i] = props[i-1].major_axis_length\n",
    "            axis_minor[iter_i] = props[i-1].minor_axis_length\n",
    "            solidity[iter_i] = props[i-1].solidity\n",
    "            \n",
    "            iter_i +=1\n",
    "    \n",
    "    # Drop empty rows: some patches values were not calculated because their size is too small/big \n",
    "    if iter_i != range_s: # Some patches were not considered -> we have to drop those rows\n",
    "        mean_intensity = mean_intensity[:iter_i]\n",
    "        columnname = columnname[:iter_i]\n",
    "        blur_lapl = blur_lapl[:iter_i]\n",
    "        blur_ski = blur_ski[:iter_i]\n",
    "        patch_size = patch_size[:iter_i]\n",
    "        sum_intensity = sum_intensity[:iter_i]\n",
    "        median_intensity = median_intensity[:iter_i]\n",
    "        imgs = imgs[:iter_i]\n",
    "        std_intensity = std_intensity[:iter_i] \n",
    "        var_intensity = var_intensity[:iter_i] \n",
    "        skew_intesity = skew_intesity[:iter_i] \n",
    "        kurt_intesity = kurt_intesity[:iter_i] \n",
    "        iqr_intesity = iqr_intesity[:iter_i]\n",
    "        entropy_intesity = entropy_intesity[:iter_i] \n",
    "        gabor_features = gabor_features[:iter_i] \n",
    "        lbp_features = lbp_features[:iter_i] \n",
    "        \n",
    "        perimeter = perimeter[:iter_i]\n",
    "        feret_diameter_max = feret_diameter_max[:iter_i]\n",
    "        area_test = area_test[:iter_i]\n",
    "        axis_major = axis_major[:iter_i]\n",
    "        axis_minor = axis_minor[:iter_i]\n",
    "        solidity = solidity[:iter_i]\n",
    "        \n",
    "    ypet_intensity = pd.DataFrame(index=columnname)\n",
    "    ypet_intensity['img'] = imgs\n",
    "    ypet_intensity['TF_name'] = TF_name\n",
    "    ypet_intensity['blur_lapl'] = blur_lapl\n",
    "    ypet_intensity['blur_ski'] = blur_ski\n",
    "    ypet_intensity['patch_size'] = patch_size\n",
    "    ypet_intensity['mean_intensity'] = mean_intensity\n",
    "    ypet_intensity['sum_intensity'] = sum_intensity\n",
    "    ypet_intensity['median_intensity'] = median_intensity\n",
    "    \n",
    "    ypet_intensity['standard_deviation'] = std_intensity \n",
    "    ypet_intensity['variance'] = var_intensity\n",
    "    ypet_intensity['skewness'] = skew_intesity\n",
    "    ypet_intensity['kurtosis'] = kurt_intesity\n",
    "    ypet_intensity['interquartile_range ']= iqr_intesity\n",
    "    ypet_intensity['entropy'] = entropy_intesity\n",
    "    ypet_intensity['Perimeter'] = perimeter \n",
    "    ypet_intensity['area_Test'] = area_test\n",
    "    ypet_intensity['axis_major_length'] = axis_major\n",
    "    ypet_intensity['feret_diameter_max'] = feret_diameter_max\n",
    "    ypet_intensity['axis_minor_length'] = axis_minor\n",
    "    ypet_intensity['solidity'] = solidity\n",
    "    \n",
    "    list_of_all_images.append(ypet_intensity)\n",
    "    \n",
    "    if imgs == []:\n",
    "        print('All nucleis have been droppped. In order to extract features you need nucleus that are not by the border and not blurry.')\n",
    "    else: \n",
    "        gabor_features_tot = np.vstack(gabor_features)\n",
    "        lbp_features_tot = np.vstack(lbp_features)\n",
    "\n",
    "        for filter_bank in range (len(kernels)):\n",
    "            name = 'gabro_'+ str(filter_bank)\n",
    "            ypet_intensity[name] = gabor_features_tot[:,filter_bank]\n",
    "\n",
    "\n",
    "        for lbp_features_count in range (lbp_features_tot.shape[1]):\n",
    "            name = 'lbp_' + str(lbp_features_count)\n",
    "            ypet_intensity[name] = lbp_features_tot[:,lbp_features_count]\n",
    "                                      \n",
    "   \n",
    "    #ypet_intensity_glob = pd.concat([ypet_intensity_glob,ypet_intensity])\n",
    "    print(\"step \"+str(j)+\" over \"+str(nb_images)+ \" is done!\")\n",
    "    \n",
    "whole array = pd.concat(list_of_all_images)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b696c2-590f-46d6-8da2-fb3fd76a2629",
   "metadata": {},
   "source": [
    "### Add column with circularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4537893-fc17-4846-a95c-63af98ea3fcf",
   "metadata": {},
   "source": [
    "Formula for circularity: 4*pi*area/(perimeter^2)\n",
    "Zero would mean that it is a circle. Everything else is between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a1e9e-5bfc-4cdc-ad29-7f61910cea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypet_intensity['Circularity'] = (4*np.pi*ypet_intensity['patch_size'])/(ypet_intensity['Perimeter']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37dd8d-1aa0-428f-80d4-51c7cbe520a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypet_intensity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68467e-d5b4-4902-abde-71de6a10ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "path = '/content/drive/MyDrive/Project_2_ml/20151218_E14_BM_mTF8additionals_4/Features/features_cells_TF8_A1.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "    ypet_intensity_glob.to_csv(f)\n",
    "\n",
    "\"\"\"  Was just to create the ~800 folders easier :)\n",
    "import os\n",
    "alphabet = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\n",
    "for k in range(8):\n",
    "  for j in range(8):\n",
    "    for i in range(12):\n",
    "      os.mkdir(\"/content/gdrive/MyDrive/ML_2_transcripted_cherry/masks_TF\"+str(k+1)+\"/\"+alphabet[j]+\"_\"+str(i+1))\n",
    "\"\"\"\n",
    "\n",
    " to remove the \"copie\" part of it once I did copy\n",
    "import os\n",
    "os.rename(\"/content/gdrive/MyDrive/ML_2_transcripted_cherry/masks_TF1/A_1/Copie de img_1_seg.npy\",\"/content/gdrive/MyDrive/ML_2_transcripted_cherry/masks_TF1/A_1/img_1_seg.npy\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e811a30-a060-4c38-9c49-8981ad081738",
   "metadata": {},
   "source": [
    "***\n",
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba66c31-7080-43e6-bab1-d7318e5a3861",
   "metadata": {},
   "source": [
    "### Outlier removal\n",
    "\n",
    "To adapt is the columns where we want to change the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58461b-0c8a-4543-960a-31365bdd98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_outliers = ['mean_intensity','patch_size']\n",
    "remove_outliers = ypet_intensity.copy()\n",
    "\n",
    "def outlier_removal(without_outliers, outlier_range = 1.5):\n",
    "    columns_outliers= ['mean_intensity','patch_size']    \n",
    "    # Use a for loop to iterate over the columns of the dataframe\n",
    "    for col in columns_outliers:\n",
    "        # Look up the threshold value for the current column using the dictionary\n",
    "        median = without_outliers[col].median()\n",
    "        q_75 = without_outliers[col].quantile(q = 0.75)\n",
    "        q_25 = without_outliers[col].quantile(q = 0.25)\n",
    "        interquantile = q_75 - q_25                              \n",
    "        upper_bound = median + (interquantile * outlier_range)\n",
    "        lower_bound = median - (interquantile * outlier_range)\n",
    "\n",
    "      # Create a boolean mask that is True for rows with a value less than or equal to the threshold\n",
    "        mask = (without_outliers[col] <= upper_bound) & (without_outliers[col] >= lower_bound)\n",
    "      # Use the mask to filter the dataframe and keep only the rows with a value less than or equal to the threshold\n",
    "        without_outliers = without_outliers[mask]\n",
    "        \n",
    "    return without_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cc466-5487-490c-9a5f-d9e2b5b8e88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "without_outliers = remove_outliers.groupby('TF_name',as_index=False).apply(outlier_removal).reset_index()\n",
    "without_outliers = without_outliers.drop(['level_0'],axis=1)\n",
    "without_outliers = without_outliers.set_index('level_1')\n",
    "without_outliers.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c9a3b-3eff-4174-9aef-227a38faf72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypet_intensity_processed = without_outliers.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf42e31-7543-4217-9293-de18eb9b207a",
   "metadata": {},
   "source": [
    "### Normalize ypet_intensity for each TF after we've concenated all df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4d9eb-d914-4289-8961-e3d15d1c37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize \n",
    "rows_to_be_normalized = ['mean_intensity', 'median_intensity']\n",
    "TF_grouped = ypet_intensity_processed.groupby('TF_name')\n",
    "Normalized_rows = TF_grouped[rows_to_be_normalized].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "# Replace normalized\n",
    "ypet_intensity_processed[rows_to_be_normalized] = Normalized_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfeaac-5b14-4e92-88ee-df8dae8db47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypet_intensity_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf785f-06cf-463a-9821-ccb5b13cbb95",
   "metadata": {},
   "source": [
    "### pca of ypet_intensity, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2aa5f0-d8b7-4c43-9e09-ce18d240b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc004ab9-6161-4398-821b-154dad273c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components= 2\n",
    "pca = PCA(n_components)\n",
    "\n",
    "# define which columns should be excluded\n",
    "pca_excluded = ['img', 'TF_name']\n",
    "to_test = ypet_intensity.drop(pca_excluded, axis = 1)\n",
    "\n",
    "# Check how much variance the PCA components explain\n",
    "pca_result = pca.fit(to_test)\n",
    "print(pca_result.explained_variance_ratio_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42123cd7-639e-4ac2-96cb-f99dedfa08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the dimensionality reduction on data\n",
    "pca_components = pca.fit_transform(to_test)\n",
    "\n",
    "# Combine excluded data and pca\n",
    "#ypet_intensity[pca_excluded].merge(ypet_intensity_pca)\n",
    "ypet_intensity_pca = ypet_intensity[pca_excluded]\n",
    "for iter_i in range(n_components):\n",
    "    name = 'pca_component_' + str(iter_i)\n",
    "    ypet_intensity_pca[name] = pca_components[:,iter_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811621a8-2e96-401f-8e17-c0b67610741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypet_intensity_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f6092-1b03-4699-86b5-3bbdbdd0de59",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766179b-b21f-4caa-9f79-281d6dbf5585",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc942ab-3375-4722-a257-ece993dfcf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation with polynomial expansion\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85217c8a-5f90-45bb-b8ec-e43b387f03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train test\n",
    "df_train, df_test = train_test_split(ypet_intensity, train_size = 0.8, test_size = 0.2, random_state = 10)\n",
    "\n",
    "testfeature = ['mean_intensity', 'sum_intensity', 'median_intensity']\n",
    "\n",
    "df_train_x = df_train[[testfeature[0]]]\n",
    "df_test_x = df_test[[testfeature[0]]]\n",
    "\n",
    "df_train_y = df_train[['patch_size']]\n",
    "df_test_y = df_test[['patch_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9c9df-7259-4808-b1e3-79720eb6c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model to select\n",
    "maxdegree = 5 #To define is the maximum degree of the polynomial\n",
    "cross_validation_ridge_error = np.zeros(maxdegree-1)\n",
    "cross_validation_lm_error = np.zeros(maxdegree-1)\n",
    "\n",
    "#see which degree fits data the best for linear regression\n",
    "for d in range(1, maxdegree):\n",
    "    #polynomial feature expansion of x_train\n",
    "    x_poly_train = PolynomialFeatures(degree=d).fit_transform(df_train_x)\n",
    "    #apply linear regression model and cross-validation for alpha-ridge regression\n",
    "    lr = LinearRegression()\n",
    "    rr = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
    "    #apply cross validation\n",
    "    cve = cross_validate(lr,x_poly_train,df_train_y,scoring='neg_mean_squared_error', cv=5, return_train_score=True)\n",
    "    crr = cross_validate(rr,x_poly_train,df_train_y,scoring='neg_mean_squared_error', cv=5, return_train_score=True)\n",
    "    #make array for cross validation with linear model and for ridge regression\n",
    "    cross_validation_lm_error[d-1] = np.mean(np.absolute(cve['test_score']))\n",
    "    cross_validation_ridge_error[d-1] = np.mean(np.absolute(crr['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b7444-d516-41ce-a05d-3885e3c6fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min_lm = np.argmin(cross_validation_lm_error)\n",
    "index_min_ridge = np.argmin(cross_validation_ridge_error)\n",
    "x_poly_test = PolynomialFeatures(degree=index_min_lm+1).fit_transform(df_test_x)\n",
    "x_poly_train = PolynomialFeatures(degree=index_min_lm+1).fit_transform(df_train_x)\n",
    "\n",
    "x_poly_train_df = pd.DataFrame(x_poly_train)\n",
    "x_poly_test_df = pd.DataFrame(x_poly_test)\n",
    "\n",
    "#make linear model\n",
    "model_lm = LinearRegression().fit(x_poly_train_df, df_train_y)\n",
    "model_ridge = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(x_poly_train_df, df_train_y)\n",
    "\n",
    "#train error\n",
    "y_train_pred_lm = model_lm.predict(x_poly_train)\n",
    "mse_train_lm = mean_squared_error(df_train_y,y_train_pred_lm)\n",
    "y_train_pred_ridge = model_ridge.predict(x_poly_train)\n",
    "mse_train_ridge = mean_squared_error(df_train_y,y_train_pred_ridge)\n",
    "\n",
    "#test error\n",
    "y_test_pred_lm = model_lm.predict(x_poly_test_df)\n",
    "mse_test_lm = mean_squared_error(df_test_y,y_test_pred_lm)\n",
    "y_test_pred_ridge = model_ridge.predict(x_poly_test_df)\n",
    "mse_test_ridge = mean_squared_error(df_test_y,y_test_pred_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43919569-b65d-482b-b8fe-bddc67e015f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"patch_size\", y=\"mean_intensity\", data=ypet_intensity, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb11f6b-33b8-4509-90b1-f4e40bdd0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show predicted things\n",
    "plt.plot(df_train_y,df_train_x, 'ro')\n",
    "plt.plot(y_test_pred_lm,df_test_x, 'ro',color = 'black')\n",
    "#plt.yscale(\"log\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ee48f-e627-4aaf-b1bc-fb64ccfb9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show predicted things\n",
    "plt.plot(df_train_y,df_train_x, 'ro')\n",
    "plt.plot(y_test_pred_ridge,df_test_x, 'ro',color = 'black')\n",
    "#plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097be32-614e-4d88-8be9-cd4bd1718a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mse_test_ridge >= mse_test_lm:\n",
    "    print('take linear regression')\n",
    "    print('coefficients: ',model_lm.coef_)\n",
    "    print('intercept: ',model_lm.intercept_)\n",
    "    \n",
    "else:\n",
    "    print('take ridge regression')\n",
    "    print('coefficients: ',model_ridge.coef_)\n",
    "    print('intercept: ',model_ridge.intercept_)\n",
    "    print('alpha: ',model_ridge.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54637200-f195-49ed-b046-7484394f43ec",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0597516-f52b-45ef-a9f9-0d2b83fc6fa6",
   "metadata": {},
   "source": [
    "## Task 2: Unsupervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba0574-c3e7-4fe8-8e07-ba24d01868a6",
   "metadata": {},
   "source": [
    "Use pca or standard df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39e993-c3a0-47a5-889c-e9652fd538fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_feature = ['patch_size', 'sum_intensity']\n",
    "ypet_intensity[clustering_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fc6f1-2df9-40eb-a2d9-299ed96a238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypet_intensity_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5faea-ce8c-429a-9422-51d64da47e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypet_cluster = ypet_intensity_pca.reset_index().set_index(['index','img', 'TF_name'])\n",
    "ypet_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93875fb9-b18c-4938-84a8-2f37d261b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11608a5e-3ddb-42e5-8fd0-4e355786e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "n_clusters = 5 #10 # Number of descriptions we want for TF\n",
    "kmean = KMeans(n_clusters=n_clusters, random_state=random_state).fit(ypet_cluster)\n",
    "\n",
    "ypet_clustered = ypet_cluster.copy()\n",
    "ypet_clustered['cluster'] = kmean.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f400e10-60c2-4729-9671-f86321914cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ypet_cluster.iloc[:,0], ypet_cluster.iloc[:,1], c=kmean.labels_, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c627c-c6a2-4c65-9a33-897c368e344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find more information\n",
    "min_n = ypet_clustered.groupby('cluster').count().min()[0] #min number of points i n a cluster\n",
    "max_n = ypet_clustered.groupby('cluster').count().max()[0] #min number of points i n a cluster\n",
    "min_n, max_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e17b99-2340-4491-8d08-923636cd27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x percentage of data per cluster -> easier to analyze\n",
    "\n",
    "extracted_i = []\n",
    "x = 0.9\n",
    "\n",
    "for i in range(n_clusters):\n",
    "\n",
    "    # indices of all the points from X that belong to cluster i\n",
    "    C_i = np.where(kmean.labels_ == i)[0].tolist() \n",
    "    n_i = len(C_i) # number of points in cluster i\n",
    "\n",
    "    # indices of the points from X to be sampled from cluster i\n",
    "    sample_i = np.random.choice(C_i, math.ceil((x * n_i))).tolist()\n",
    "    extracted_i.extend(sample_i) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39eb6f0-eaf1-4819-be80-f725884692a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypet_clustered.iloc[np.unique(extracted_i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e9135-9464-4c13-b60e-a3ec2728dc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4df8ed0d-fc48-4c73-b344-c1f99adde2fa",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualizations (Exploration phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2543c40-4721-4578-8f4e-f3daf09c0d0b",
   "metadata": {},
   "source": [
    "#### Tune blur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987e2b1-3c0e-4bdb-ba68-bb1c84fe5d65",
   "metadata": {},
   "source": [
    "plt.imshow(cv2.Laplacian(variance_data_list[test], cv2.CV_64F), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bb17d-59fb-48b6-888a-5f6b12e24684",
   "metadata": {},
   "source": [
    "for i in range(1,100), image E-7\n",
    "\n",
    "Not blurred:\n",
    "variance_data_list[-1] -> 1'763'695\n",
    "variance_data_list[-10] -> 3'056'590\n",
    "variance_data_list[-10] -> 3'056'590\n",
    "[-17] -> 1'056'899\n",
    "[-20] -> 567'588\n",
    "[-21] -> 2'138'449\n",
    "[-23] -> 5'134'705\n",
    "[-33] 30'821'019\n",
    "\n",
    "\n",
    "Right classified with other nucleus near:\n",
    "variance_data_list[-6] -> 25'095'309\n",
    "variance_data_list[-13] ->5'698'016\n",
    "[-16] -> 9'219'300\n",
    "[-19] -> 11'049'582\n",
    "[-25] -> 2'313'727\n",
    "[-26] -> 157'114'853\n",
    "[-27] -> 10'010'503\n",
    "[-28] -> 3'063'935\n",
    "[-31] -> 1'803'048\n",
    "9'826'707\n",
    "\n",
    "\n",
    "Wrongly classified:\n",
    "variance_data_list[-6] -> 25'095'309\n",
    "variance_data_list[-9] -> 57'898'307\n",
    "[-14] -> 21'739'451\n",
    "[-15] -> 6'769'331\n",
    "[-30] -> 12'627'256\n",
    "\n",
    "\n",
    "Overlay:\n",
    "variance_data_list[-4] -> 34'614'946\n",
    "[-22] -> 12'964'725\n",
    "[-29] -> 17'727'949\n",
    "\n",
    "Blurr/ overflow:\n",
    "[-18] -> 283'062'008\n",
    "[-24] -> 17'935'490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d7ce0-aee3-4c0d-943a-9f85acb3ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.Laplacian(variance_data_list[test], cv2.CV_64F).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3157fd-7a0e-4be7-9c36-a074dc568d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure.blur_effect(variance_data_list[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb7dcb-9e8d-4ad4-99b4-7a7834b9dd53",
   "metadata": {},
   "source": [
    "#### Mask, nucleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7d8ec-68c3-428e-9174-933d59db240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(boolean, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348021e6-ffc7-42f4-9005-17f499a0314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(patch_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4bd49-104c-4ebf-b4e3-50f88b813e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 31\n",
    "plt.imshow(variance_data_list[test], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4e51d-638c-4a13-9c3f-acf9ceee8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(patch_data_list[test], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c50429-3d9f-4f89-8f9c-21f1b5ac1f23",
   "metadata": {},
   "source": [
    "## Local binary patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f314c0-8c3a-4846-b4ec-c957326fd326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize lpb\n",
    "METHOD = 'uniform'   \n",
    "radius = 1\n",
    "n_points = 8 * radius\n",
    "image = patch_data\n",
    "\n",
    "lbp = local_binary_pattern(image, n_points, radius, METHOD)\n",
    "\n",
    "    \n",
    "def overlay_labels(image, lbp, labels):\n",
    "    mask = np.logical_or.reduce([lbp == each for each in labels])\n",
    "    return label2rgb(mask, image=image, bg_label=0, alpha=0.5)\n",
    "\n",
    "\n",
    "def highlight_bars(bars, indexes):\n",
    "    for i in indexes:\n",
    "        bars[i].set_facecolor('r')\n",
    "def hist(ax, lbp):\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    return ax.hist(lbp.ravel(), density=True, bins=n_bins, range=(0, n_bins),\n",
    "                   facecolor='0.5')\n",
    "\n",
    "\n",
    "# plot histograms of LBP of textures\n",
    "fig, (ax_img, ax_hist) = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "plt.gray()\n",
    "\n",
    "titles = ('edge', 'flat', 'corner')\n",
    "w = width = radius - 1\n",
    "edge_labels = range(n_points // 2 - w, n_points // 2 + w + 1)\n",
    "flat_labels = list(range(0, w + 1)) + list(range(n_points - w, n_points + 2))\n",
    "i_14 = n_points // 4            # 1/4th of the histogram\n",
    "i_34 = 3 * (n_points // 4)      # 3/4th of the histogram\n",
    "corner_labels = (list(range(i_14 - w, i_14 + w + 1)) +\n",
    "                 list(range(i_34 - w, i_34 + w + 1)))\n",
    "\n",
    "label_sets = (edge_labels, flat_labels, corner_labels)\n",
    "\n",
    "for ax, labels in zip(ax_img, label_sets):\n",
    "    ax.imshow(overlay_labels(image, lbp, labels))\n",
    "\n",
    "for ax, labels, name in zip(ax_hist, label_sets, titles):\n",
    "    counts, _, bars = hist(ax, lbp)\n",
    "    highlight_bars(bars, labels)\n",
    "    ax.set_ylim(top=np.max(counts[:-1]))\n",
    "    ax.set_xlim(right=n_points + 2)\n",
    "    ax.set_title(name)\n",
    "\n",
    "ax_hist[0].set_ylabel('Percentage')\n",
    "for ax in ax_img:\n",
    "    ax.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974dac6a-d215-4d1a-a8b9-945aaa552a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove background    \n",
    "\n",
    "# plot histograms of LBP of textures\n",
    "fig, (ax_img, ax_hist) = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "plt.gray()\n",
    "\n",
    "titles = ('edge', 'flat', 'corner')\n",
    "w = width = radius - 1\n",
    "edge_labels = range(n_points // 2 - w, n_points // 2 + w + 1)\n",
    "flat_labels = list(range(0, w + 1)) # Remove background: list(range(n_points - w, n_points + 2))\n",
    "i_14 = n_points // 4            # 1/4th of the histogram\n",
    "i_34 = 3 * (n_points // 4)      # 3/4th of the histogram\n",
    "corner_labels = (list(range(i_14 - w, i_14 + w + 1)) +\n",
    "                 list(range(i_34 - w, i_34 + w + 1)))\n",
    "\n",
    "label_sets = (edge_labels, flat_labels, corner_labels)\n",
    "\n",
    "\n",
    "\n",
    "for ax, labels in zip(ax_img, label_sets):\n",
    "    ax.imshow(overlay_labels(image, lbp, labels))\n",
    "\n",
    "for ax, labels, name in zip(ax_hist, label_sets, titles):\n",
    "    counts, _, bars = hist(ax, lbp)   \n",
    "    highlight_bars(bars, labels)\n",
    "    ax.set_ylim(top=np.max(counts[:-1]))\n",
    "    ax.set_xlim(right=n_points + 2)\n",
    "    ax.set_title(name)\n",
    "\n",
    "ax_hist[0].set_ylabel('Percentage')\n",
    "for ax in ax_img:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Drop background\n",
    "count = len(np.unique(lbp))\n",
    "H, bins = np.histogram(lbp, count)\n",
    "\n",
    "# Drop background values\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "index_background = n_points - w\n",
    "H = H[0: index_background]\n",
    "bins =  bins[0: index_background]\n",
    "\n",
    "plt.bar(bins,H,width=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "158pHXq8dG2em_g3xFihk0nOTNy6ecxOY",
     "timestamp": 1670264238282
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
